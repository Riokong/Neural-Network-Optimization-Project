{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Fundamentals of Neural Networks\n",
    "### Course: CS 5480 (Deep Learning) @ Missouri S&T\n",
    "### Semester: Spring 2025\n",
    "### Template Scribe: Dr. Sid Nadendla\n",
    "## Author: <Type your name here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "import numpy as np\n",
    "\n",
    "# sklearn has the MNIST dataset, as well as the supporting functions to preprocess data\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z,W):   \n",
    "    # TODO: Implement linear function\n",
    "    \"\"\"\n",
    "    Computes the linear function Y = W * X\n",
    "    \n",
    "    Parameters:\n",
    "    x (numpy.ndarray): K x 1 vector\n",
    "    W (numpy.ndarray): M x K matrix\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: M x 1 vector Y\n",
    "    \"\"\"\n",
    "    return np.dot(W, z) \n",
    "    \n",
    "def linear_grad(z,W,mode=\"default\"):\n",
    "    # TODO: Implement gradient of the linear function -- need to return a different output depending on the mode. \n",
    "    # A default mode computes the gradient with respect to the input z, while the weight mode computes the gradient with respect to the weight W.\n",
    "    M, K = W.shape\n",
    "\n",
    "    if mode == \"default\":\n",
    "        # Gradient with respect to z is W (M x X)\n",
    "        return W\n",
    "    elif mode == \"weight\":\n",
    "        # Gradient with respect to W is M x M x K tensor\n",
    "        grad_W = np.zeros(M, M, K)\n",
    "        for i in range(M):\n",
    "            grad_W[i, i, :] = z.flatten() # z.flatten(), converts the column vector z(of shape K x 1 into a 1D array of shape K\n",
    "        return grad_W\n",
    "    else:\n",
    "        raise NotImplementedError(\"Gradient of linear function not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # TODO: Implement ReLU activation\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_grad(y):\n",
    "    # TODO: Implement the gradient of ReLU activation as a function of output\n",
    "    \"\"\"\n",
    "        Computes the subgradient of the ReLU activation, \n",
    "        Returns an M × M dimensional matrix\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "  \n",
    "    M = y.shape[0]\n",
    "    grad = np.zeros((M, M))\n",
    "    for i in range(M):\n",
    "        grad[i, i] = 1 if np.all(y[i] > 0) else 0\n",
    "    return grad\n",
    "\"\"\" \n",
    "    #M = y.shape[0]\n",
    "    #grad = np.diag((y > 0).astype(float))\n",
    "    #return grad\n",
    "    return (y > 0).astype(float) \n",
    "    \n",
    "def logistic(x):\n",
    "    # TODO: Implement logistic activation\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_grad(y):\n",
    "    # TODO: Implement the gradient of logistic activation as a function of output\n",
    "    \"\"\"\n",
    "    Computes the gradient of the logistic function.\n",
    "    Returns an M × M dimensional matrix with negative values.\n",
    "    \"\"\"\n",
    "    return np.diag(y.flatten() * (1 - y.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(y,y_hat):\n",
    "    # TODO: Implement forward pass of the loss function\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    y (float): True label (0 or 1)\n",
    "    y_hat (float): Predicted probability (0 < y_hat < 1)\n",
    "    \n",
    "    Returns:\n",
    "    float: Gradient of binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    if not (y in [0, 1] and 0 < y_hat < 1):\n",
    "        raise ValueError(\"Both y must be 0 or 1, and y_hat must be between 0 and 1 (exclusive).\")\n",
    "    \n",
    "    return - (y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "def crossentropy_grad(y,y_hat):\n",
    "    \n",
    "    # TODO: Implement gradient of the loss function\n",
    "    delta = 1e-6\n",
    "    if not (np.all(np.isin(y, [0, 1])) and np.all((0 < y_hat) & (y_hat < 1))):\n",
    "        raise ValueError(\"Both y must be 0 or 1, and y_hat must be between 0 and 1 (exclusive).\")\n",
    "    \n",
    "    return (1 - y + delta) / (1 - y_hat + delta) - (y + delta) / (y_hat + delta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell only to test the code written for Problem 1.\n",
    "def test_problem1():\n",
    "    \"\"\"Unit tests for Problem 1 implementations.\"\"\"\n",
    "    M, K = 3, 2\n",
    "    W = np.array([[1, 0], [0, 1], [1, -1]])\n",
    "    x = np.array([[1], [2]])\n",
    "    \n",
    "    # Test linear function\n",
    "    expected_linear = np.array([[1], [2], [-1]])\n",
    "    assert np.array_equal(linear(x, W), expected_linear), \"Linear function failed\"\n",
    "    \n",
    "    # Test ReLU function\n",
    "    expected_relu = np.array([[1], [2], [0]])\n",
    "    assert np.array_equal(relu(expected_linear), expected_relu), \"ReLU function failed\"\n",
    "    \n",
    "    # Test ReLU gradient\n",
    "    y = np.array([1, 2, -1])\n",
    "    expected_output = np.array([1, 1, 0])\n",
    "    assert np.array_equal(relu_grad(y), expected_output), \"1D input test failed\"\n",
    "\n",
    "    # 2D input\n",
    "    y = np.array([[1, 2], [3, -1]])\n",
    "    expected_output = np.array([[1, 1], [1, 0]])\n",
    "    assert np.array_equal(relu_grad(y), expected_output), \"2D input test failed\"\n",
    "\n",
    "    \n",
    "    # Test logistic function\n",
    "    assert np.allclose(logistic(0), 0.5), \"Logistic function failed\"\n",
    "    \n",
    "    # Test crossentropy function\n",
    "    assert np.allclose(crossentropy_grad(1, 0.9), (1 - 1) / (1 - 0.9 + 1e-6) - (1 + 1e-6) / (0.9 + 1e-6)), \"Crossentropy gradient failed\"\n",
    "    \n",
    "    print(\"All tests passed successfully!\")\n",
    "   \n",
    "\n",
    "test_problem1()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: NN-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract class for one layer in NN (DO NOT CHANGE THIS CELL)\n",
    "class Layer():\n",
    "    def __init__(self, W, activation):\n",
    "        self.W  = W\n",
    "        self.activation = activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2:\n",
    "    #def __init__(self):\n",
    "    def __init__(self, M, K):\n",
    "        # Two layers in the model architecture\n",
    "        self.layers = [\n",
    "            #[Layer(None, relu), \n",
    "            #Layer(None, logistic)]\n",
    "            Layer(None, relu), \n",
    "            #Layer(W0, relu),\n",
    "            #Layer(W1, logistic)]\n",
    "            Layer(None, logistic)]\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # TODO: Implement the forward pass of the model and return the model output\n",
    "        if self.layers[0].W is None or self.layers[1].W is None:\n",
    "            raise ValueError(\"Weights must be initialized before forward pass.\")\n",
    "\n",
    "        self.z1 = linear(x, self.layers[0].W)\n",
    "        self.z1_tilde = self.layers[0].activation(self.z1)\n",
    "        \n",
    "        self.z2 = linear(self.z1_tilde, self.layers[1].W.T)\n",
    "        #self.z2 = linear(self.z1_tilde, self.layers[1].W)\n",
    "        self.y_hat = self.layers[1].activation(self.z2)\n",
    "        return self.y_hat\n",
    "\n",
    "\n",
    "    def backward(self,x, y): # made a change here, initially is (self, x)\n",
    "        # TODO: Implement the backprop algorithm to compute the model gradients\n",
    "        if self.layers[0].W is None or self.layers[1].W is None:\n",
    "            raise ValueError(\"Weights must be initialized before backward pass.\")\n",
    "\n",
    "        N = x.shape[1]\n",
    "        y_hat = self.forward(x)\n",
    "        \n",
    "        dL_dy_hat = crossentropy_grad(y, y_hat)\n",
    "        dy_hat_dz2 = self.y_hat * (1 - self.y_hat)\n",
    "        dL_dz2 = dL_dy_hat * dy_hat_dz2\n",
    "\n",
    "        dL_dw2 = np.dot(self.z1_tilde, dL_dz2.T) / N # np.dot((M, N), (N, 1)) \n",
    "        #dL_dw2 = np.dot(self.z1_tilde, dL_dz2) / N # np.dot((M, 1), (1, 1)) \n",
    "        dz2_dz1_tilde = self.layers[1].W\n",
    "        dz1_tilde_dz1 = relu_grad(self.z1)\n",
    "        dL_dz1 = np.dot(dz2_dz1_tilde, dL_dz2) * dz1_tilde_dz1\n",
    "        #dL_dz1 = np.dot(dz2_dz1_tilde, dL_dz2.T) * dz1_tilde_dz1\n",
    "        dL_dW1 = np.dot(dL_dz1, x.T) / N  # x.T, a little bit not sure of why x.T\n",
    "\n",
    "        return dL_dW1, dL_dw2\n",
    "        \n",
    "    def emp_loss_grad(self, train_X, train_y):\n",
    "        # TODO: Implement the empirical loss gradients for the training set\n",
    "        return self.backward(train_X, train_y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Optimization Algorithm - Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added a helper function \n",
    "def compute_loss(y, y_hat, delta=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the average binary cross-entropy loss over all samples.\n",
    "    \n",
    "    Parameters:\n",
    "    y (numpy.ndarray): True labels with shape (1, N)\n",
    "    y_hat (numpy.ndarray): Predicted probabilities with shape (1, N)\n",
    "    delta (float): A small constant to avoid log(0)\n",
    "    \n",
    "    Returns:\n",
    "    float: The average loss over all N samples.\n",
    "    \"\"\"\n",
    "    loss = - (y * np.log(y_hat + delta) + (1 - y) * np.log(1 - y_hat + delta))\n",
    "    return np.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(model, train_X, train_y, lr, R):  \n",
    "    # TODO: Implement gradient descent and return updated weights\n",
    "    prev_loss = float('inf')\n",
    "    for r in range(R):\n",
    "        # Forward pass: get predictions on the full training set\n",
    "        y_hat = model.forward(train_X)\n",
    "        # Compute the average cross-entropy loss\n",
    "        loss = compute_loss(train_y, y_hat)\n",
    "        print(f\"Iteration {r}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Compute gradients via backpropagation\n",
    "        dL_dW1, dL_dw2 = model.emp_loss_grad(train_X, train_y)\n",
    "        \n",
    "        # Update the weights\n",
    "        model.layers[0].W -= lr * dL_dW1\n",
    "        model.layers[1].W -= lr * dL_dw2\n",
    "        \n",
    "        # Convergence check based on loss change\n",
    "        if abs(prev_loss - loss) < 1e-6:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        \n",
    "    return model   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.4370\n",
      "Iteration 1, Loss: 0.4367\n",
      "Iteration 2, Loss: 0.4364\n",
      "Iteration 3, Loss: 0.4361\n",
      "Iteration 4, Loss: 0.4358\n",
      "Iteration 5, Loss: 0.4354\n",
      "Iteration 6, Loss: 0.4351\n",
      "Iteration 7, Loss: 0.4348\n",
      "Iteration 8, Loss: 0.4345\n",
      "Iteration 9, Loss: 0.4342\n",
      "Iteration 10, Loss: 0.4338\n",
      "Iteration 11, Loss: 0.4335\n",
      "Iteration 12, Loss: 0.4332\n",
      "Iteration 13, Loss: 0.4329\n",
      "Iteration 14, Loss: 0.4326\n",
      "Iteration 15, Loss: 0.4322\n",
      "Iteration 16, Loss: 0.4319\n",
      "Iteration 17, Loss: 0.4316\n",
      "Iteration 18, Loss: 0.4313\n",
      "Iteration 19, Loss: 0.4309\n",
      "Iteration 20, Loss: 0.4306\n",
      "Iteration 21, Loss: 0.4303\n",
      "Iteration 22, Loss: 0.4300\n",
      "Iteration 23, Loss: 0.4296\n",
      "Iteration 24, Loss: 0.4293\n",
      "Iteration 25, Loss: 0.4290\n",
      "Iteration 26, Loss: 0.4287\n",
      "Iteration 27, Loss: 0.4283\n",
      "Iteration 28, Loss: 0.4280\n",
      "Iteration 29, Loss: 0.4277\n",
      "Iteration 30, Loss: 0.4273\n",
      "Iteration 31, Loss: 0.4270\n",
      "Iteration 32, Loss: 0.4267\n",
      "Iteration 33, Loss: 0.4263\n",
      "Iteration 34, Loss: 0.4260\n",
      "Iteration 35, Loss: 0.4257\n",
      "Iteration 36, Loss: 0.4253\n",
      "Iteration 37, Loss: 0.4250\n",
      "Iteration 38, Loss: 0.4247\n",
      "Iteration 39, Loss: 0.4243\n",
      "Iteration 40, Loss: 0.4240\n",
      "Iteration 41, Loss: 0.4237\n",
      "Iteration 42, Loss: 0.4233\n",
      "Iteration 43, Loss: 0.4230\n",
      "Iteration 44, Loss: 0.4227\n",
      "Iteration 45, Loss: 0.4223\n",
      "Iteration 46, Loss: 0.4220\n",
      "Iteration 47, Loss: 0.4217\n",
      "Iteration 48, Loss: 0.4213\n",
      "Iteration 49, Loss: 0.4210\n",
      "Gradient descent test passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell only to test the code written for Problem 2.\n",
    "def test_problem2():\n",
    "    \n",
    "    \n",
    "    \"\"\"Unit tests for gradient descent implementation.\"\"\"\n",
    "    M, K, N = 4, 3, 5  # Example sizes\n",
    "    model = NN2(M, K)  # Pass M and K when initializing\n",
    "\n",
    "    model.layers[0].W = np.random.randn(M, K)\n",
    "    model.layers[1].W = np.random.randn(M, 1)\n",
    "    \n",
    "    X = np.random.randn(K, N)\n",
    "    Y = (np.random.rand(1, N) > 0.5).astype(float)  # Binary labels\n",
    "\n",
    "    model = gradient_descent(model, X, Y, lr=0.01, R=50)\n",
    "    \n",
    "    assert model.layers[0].W is not None, \"W1 update failed\"\n",
    "    assert model.layers[1].W is not None, \"w2 update failed\"\n",
    "    print(\"Gradient descent test passed!\")\n",
    "\n",
    "# Run the fixed test\n",
    "test_problem2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4: Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    #TODO: Return the data tuple as input (x) and output (y) \n",
    "    \"\"\"Load the MNIST dataset.\"\"\"\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    return X, y\n",
    "\n",
    "def prepare_data(X, y):\n",
    "    #TODO: Convert output labels in MNIST into binary labels\n",
    "    \"\"\"Normalize and preprocess MNIST dataset.\"\"\"\n",
    "    # Normalize pixel values (0 to 1)\n",
    "    X = X / 255.0\n",
    "    \n",
    "    # Flatten each image into a 784 x 1 vector and append '-1' as bias term\n",
    "    X = np.hstack([X, -np.ones((X.shape[0], 1))])  # Now X has shape (70000, 785)\n",
    "    \n",
    "    # Convert labels into binary classification: Even digits -> 1, Odd digits -> 0\n",
    "    y = (y % 2 == 0).astype(int)\n",
    "    \n",
    "    return X, y # change X to X.t (785, 70000)\n",
    "    \n",
    "def split_train_test(X,y):\n",
    "    #TODO: return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    return train_test_split(X, y, test_size=10000/70000, random_state=42)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "def initialize_model(X_train, X_test, y_train, y_test):\n",
    "    #TODO\n",
    "\n",
    "    M = 128  # Number of hidden units\n",
    "    K = X_train.shape[0]  # Input dimension (785 after adding bias)\n",
    "    W0 = np.random.randn(M, K) * np.sqrt(1.0 / K) # change 0.01 to np.sqrt(1.0 / K )\n",
    "    W1 = np.random.randn(M, 1) * np.sqrt(1.0 / M)\n",
    "    \n",
    "\n",
    "    print(f\"Size of W0: {W0.shape}, Size of W1: {W1.shape}\")\n",
    "\n",
    "    \n",
    "    #two_layer_nn  = NN2()\n",
    "    two_layer_nn = NN2(M, K)  # Initialize the model\n",
    "    two_layer_nn.W = [W0, W1]\n",
    "    two_layer_nn.layers[0].W = W0\n",
    "    two_layer_nn.layers[1].W = W1\n",
    "\n",
    "    return two_layer_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train):\n",
    "    #TODO: Use GD to train the model on training dataset\n",
    "    \n",
    "    model = gradient_descent(model, X_train, y_train.reshape(1, -1), lr=0.01, R= 100)\n",
    "    final_W = [model.layers[0].W, model.layers[1].W]  # add this\n",
    "    return model, final_W # add final_W\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test): \n",
    "    #TODO: Define accuracy metric and return the accuracy of the trained model on test data\n",
    "    \n",
    "    \"\"\"Evaluate model accuracy on the test dataset.\"\"\"\n",
    "    y_pred = model.forward(X_test)  # Forward pass to get predictions\n",
    "    y_pred_labels = (y_pred > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "    accuracy = np.mean(y_pred_labels == y_test.reshape(1, -1)) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Run this cell when the entire code-base is ready for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of W0: (128, 785), Size of W1: (128, 1)\n",
      "Iteration 0, Loss: 0.7028\n",
      "Iteration 1, Loss: 0.7000\n",
      "Iteration 2, Loss: 0.6973\n",
      "Iteration 3, Loss: 0.6948\n",
      "Iteration 4, Loss: 0.6923\n",
      "Iteration 5, Loss: 0.6899\n",
      "Iteration 6, Loss: 0.6876\n",
      "Iteration 7, Loss: 0.6853\n",
      "Iteration 8, Loss: 0.6831\n",
      "Iteration 9, Loss: 0.6810\n",
      "Iteration 10, Loss: 0.6789\n",
      "Iteration 11, Loss: 0.6769\n",
      "Iteration 12, Loss: 0.6749\n",
      "Iteration 13, Loss: 0.6729\n",
      "Iteration 14, Loss: 0.6710\n",
      "Iteration 15, Loss: 0.6691\n",
      "Iteration 16, Loss: 0.6673\n",
      "Iteration 17, Loss: 0.6655\n",
      "Iteration 18, Loss: 0.6637\n",
      "Iteration 19, Loss: 0.6619\n",
      "Iteration 20, Loss: 0.6601\n",
      "Iteration 21, Loss: 0.6584\n",
      "Iteration 22, Loss: 0.6567\n",
      "Iteration 23, Loss: 0.6550\n",
      "Iteration 24, Loss: 0.6533\n",
      "Iteration 25, Loss: 0.6517\n",
      "Iteration 26, Loss: 0.6500\n",
      "Iteration 27, Loss: 0.6484\n",
      "Iteration 28, Loss: 0.6468\n",
      "Iteration 29, Loss: 0.6452\n",
      "Iteration 30, Loss: 0.6436\n",
      "Iteration 31, Loss: 0.6420\n",
      "Iteration 32, Loss: 0.6404\n",
      "Iteration 33, Loss: 0.6389\n",
      "Iteration 34, Loss: 0.6373\n",
      "Iteration 35, Loss: 0.6358\n",
      "Iteration 36, Loss: 0.6342\n",
      "Iteration 37, Loss: 0.6327\n",
      "Iteration 38, Loss: 0.6312\n",
      "Iteration 39, Loss: 0.6297\n",
      "Iteration 40, Loss: 0.6282\n",
      "Iteration 41, Loss: 0.6267\n",
      "Iteration 42, Loss: 0.6252\n",
      "Iteration 43, Loss: 0.6237\n",
      "Iteration 44, Loss: 0.6222\n",
      "Iteration 45, Loss: 0.6208\n",
      "Iteration 46, Loss: 0.6193\n",
      "Iteration 47, Loss: 0.6178\n",
      "Iteration 48, Loss: 0.6164\n",
      "Iteration 49, Loss: 0.6149\n",
      "Iteration 50, Loss: 0.6135\n",
      "Iteration 51, Loss: 0.6121\n",
      "Iteration 52, Loss: 0.6106\n",
      "Iteration 53, Loss: 0.6092\n",
      "Iteration 54, Loss: 0.6078\n",
      "Iteration 55, Loss: 0.6064\n",
      "Iteration 56, Loss: 0.6050\n",
      "Iteration 57, Loss: 0.6036\n",
      "Iteration 58, Loss: 0.6022\n",
      "Iteration 59, Loss: 0.6008\n",
      "Iteration 60, Loss: 0.5994\n",
      "Iteration 61, Loss: 0.5980\n",
      "Iteration 62, Loss: 0.5967\n",
      "Iteration 63, Loss: 0.5953\n",
      "Iteration 64, Loss: 0.5939\n",
      "Iteration 65, Loss: 0.5926\n",
      "Iteration 66, Loss: 0.5912\n",
      "Iteration 67, Loss: 0.5898\n",
      "Iteration 68, Loss: 0.5885\n",
      "Iteration 69, Loss: 0.5872\n",
      "Iteration 70, Loss: 0.5858\n",
      "Iteration 71, Loss: 0.5845\n",
      "Iteration 72, Loss: 0.5832\n",
      "Iteration 73, Loss: 0.5818\n",
      "Iteration 74, Loss: 0.5805\n",
      "Iteration 75, Loss: 0.5792\n",
      "Iteration 76, Loss: 0.5779\n",
      "Iteration 77, Loss: 0.5766\n",
      "Iteration 78, Loss: 0.5753\n",
      "Iteration 79, Loss: 0.5740\n",
      "Iteration 80, Loss: 0.5727\n",
      "Iteration 81, Loss: 0.5714\n",
      "Iteration 82, Loss: 0.5702\n",
      "Iteration 83, Loss: 0.5689\n",
      "Iteration 84, Loss: 0.5676\n",
      "Iteration 85, Loss: 0.5664\n",
      "Iteration 86, Loss: 0.5651\n",
      "Iteration 87, Loss: 0.5638\n",
      "Iteration 88, Loss: 0.5626\n",
      "Iteration 89, Loss: 0.5613\n",
      "Iteration 90, Loss: 0.5601\n",
      "Iteration 91, Loss: 0.5589\n",
      "Iteration 92, Loss: 0.5576\n",
      "Iteration 93, Loss: 0.5564\n",
      "Iteration 94, Loss: 0.5552\n",
      "Iteration 95, Loss: 0.5540\n",
      "Iteration 96, Loss: 0.5528\n",
      "Iteration 97, Loss: 0.5516\n",
      "Iteration 98, Loss: 0.5504\n",
      "Iteration 99, Loss: 0.5492\n",
      "Layer 0 weights shape: [[ 0.06611064 -0.04515404  0.03897488 ...  0.02436387 -0.00591491\n",
      "  -0.03216974]\n",
      " [ 0.08243754 -0.0236955   0.04165569 ... -0.02243159  0.02150333\n",
      "  -0.02413492]\n",
      " [-0.01821824  0.0414712  -0.02742412 ...  0.03305258  0.02339942\n",
      "  -0.05480719]\n",
      " ...\n",
      " [ 0.00184806 -0.03400091 -0.01066562 ... -0.0120541   0.00304229\n",
      "   0.01258529]\n",
      " [ 0.03439797  0.0505407   0.07687229 ... -0.02628301 -0.01867319\n",
      "  -0.09626107]\n",
      " [-0.01551277 -0.05018109  0.01485184 ... -0.01620287 -0.00779001\n",
      "   0.0541516 ]]\n",
      "Layer 1 weights shape: [[ 0.24011714]\n",
      " [ 0.14036527]\n",
      " [-0.00074737]\n",
      " [-0.0934078 ]\n",
      " [ 0.04984496]\n",
      " [ 0.15990999]\n",
      " [ 0.00126238]\n",
      " [ 0.19427032]\n",
      " [-0.19436452]\n",
      " [ 0.06823089]\n",
      " [ 0.08630253]\n",
      " [ 0.11980409]\n",
      " [ 0.01932411]\n",
      " [-0.05382215]\n",
      " [ 0.1153703 ]\n",
      " [-0.28314431]\n",
      " [-0.05236633]\n",
      " [-0.00441584]\n",
      " [ 0.01666238]\n",
      " [ 0.13484286]\n",
      " [ 0.11891091]\n",
      " [ 0.00302299]\n",
      " [-0.14959406]\n",
      " [-0.01154594]\n",
      " [-0.03835257]\n",
      " [-0.0750061 ]\n",
      " [-0.05421078]\n",
      " [ 0.04362334]\n",
      " [-0.02268241]\n",
      " [ 0.02080146]\n",
      " [-0.12366128]\n",
      " [ 0.01234677]\n",
      " [ 0.06204983]\n",
      " [ 0.0980689 ]\n",
      " [ 0.12065829]\n",
      " [-0.0874859 ]\n",
      " [ 0.09232293]\n",
      " [-0.13441949]\n",
      " [-0.28021117]\n",
      " [-0.0015456 ]\n",
      " [ 0.09741488]\n",
      " [-0.1043242 ]\n",
      " [-0.0445421 ]\n",
      " [-0.06800878]\n",
      " [ 0.15684207]\n",
      " [ 0.09678222]\n",
      " [ 0.02391182]\n",
      " [-0.09176007]\n",
      " [-0.06443552]\n",
      " [ 0.18443408]\n",
      " [ 0.02121445]\n",
      " [ 0.01326771]\n",
      " [ 0.00964145]\n",
      " [-0.02321026]\n",
      " [ 0.13404352]\n",
      " [-0.00222226]\n",
      " [-0.08845136]\n",
      " [-0.035242  ]\n",
      " [-0.05275306]\n",
      " [-0.02736674]\n",
      " [ 0.00850302]\n",
      " [-0.08078721]\n",
      " [-0.11980954]\n",
      " [ 0.02716191]\n",
      " [ 0.07866309]\n",
      " [-0.00752113]\n",
      " [-0.00293342]\n",
      " [ 0.00746504]\n",
      " [-0.01609769]\n",
      " [-0.0592465 ]\n",
      " [-0.05966627]\n",
      " [ 0.03201173]\n",
      " [-0.06466647]\n",
      " [-0.16386145]\n",
      " [ 0.01162942]\n",
      " [ 0.01205113]\n",
      " [ 0.16032464]\n",
      " [-0.02645252]\n",
      " [-0.09207749]\n",
      " [-0.02382105]\n",
      " [-0.00372674]\n",
      " [ 0.02733786]\n",
      " [-0.17746191]\n",
      " [-0.06102479]\n",
      " [-0.02743535]\n",
      " [ 0.09229549]\n",
      " [ 0.11755497]\n",
      " [ 0.01964114]\n",
      " [-0.04974528]\n",
      " [ 0.04672156]\n",
      " [ 0.10448117]\n",
      " [-0.10486965]\n",
      " [-0.05788865]\n",
      " [-0.0729071 ]\n",
      " [ 0.15659679]\n",
      " [-0.0459265 ]\n",
      " [-0.21432938]\n",
      " [ 0.04640165]\n",
      " [ 0.0945007 ]\n",
      " [-0.17374196]\n",
      " [ 0.04830771]\n",
      " [-0.15369924]\n",
      " [-0.01272094]\n",
      " [ 0.10364912]\n",
      " [ 0.07018493]\n",
      " [ 0.11278394]\n",
      " [-0.05010526]\n",
      " [ 0.0055634 ]\n",
      " [ 0.00967649]\n",
      " [-0.08880983]\n",
      " [-0.00046551]\n",
      " [ 0.01213996]\n",
      " [ 0.09378475]\n",
      " [ 0.03665814]\n",
      " [ 0.10923757]\n",
      " [ 0.0770357 ]\n",
      " [-0.16485842]\n",
      " [ 0.00783152]\n",
      " [ 0.06566644]\n",
      " [ 0.00591832]\n",
      " [ 0.02236471]\n",
      " [-0.09758514]\n",
      " [-0.03065029]\n",
      " [ 0.05164693]\n",
      " [ 0.01248471]\n",
      " [ 0.1367709 ]\n",
      " [ 0.12294523]\n",
      " [ 0.08679753]]\n",
      "Test Accuracy: 81.25%\n",
      "Completed testing model - Accuracy : 81.25\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "x, y = load_dataset()\n",
    "\n",
    "#prepare data\n",
    "x, y = prepare_data(x,y)\n",
    "\n",
    "# split data set\n",
    "X_train, X_test, y_train, y_test = split_train_test(x,y)\n",
    "\n",
    "X_train = X_train.T  # Now shape (785, n_train), Added\n",
    "X_test = X_test.T    # Now shape (785, n_test), Added\n",
    "\n",
    "#initialize model\n",
    "model = initialize_model(X_train, X_test, y_train, y_test)\n",
    "\n",
    "#training model\n",
    "trained_model, final_W = train_model(model, X_train, y_train) # add final_W\n",
    "#print(f\"Completed training model - final W : {final_W}\")\n",
    "print(\"Layer 0 weights shape:\", final_W[0]) #\n",
    "print(\"Layer 1 weights shape:\", final_W[1]) #\n",
    "\n",
    "#testing model\n",
    "accuracy = test_model(model, X_test, y_test) # delete final_W\n",
    "print(f\"Completed testing model - Accuracy : {accuracy}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus try SGD and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full-batch gradient descent with momentum\n",
    "def sgd_momentum(model, train_X, train_y, lr=0.01, momentum=0.9, R=100):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent with Momentum optimizer.\n",
    "    \n",
    "    Parameters:\n",
    "      model:  NN2 model.\n",
    "      train_X: input data of shape (K, N) where K is the number of features and N the number of samples.\n",
    "      train_y: true labels with shape (1, N).\n",
    "      lr: learning rate.\n",
    "      momentum: momentum factor.\n",
    "      R: number of iterations.\n",
    "      \n",
    "    Returns:\n",
    "      Updated model after training.\n",
    "    \"\"\"\n",
    "    # Initialize velocity for each weight matrix\n",
    "    v_W1 = np.zeros_like(model.layers[0].W)\n",
    "    v_W2 = np.zeros_like(model.layers[1].W)\n",
    "    prev_loss = float('inf')\n",
    "    \n",
    "    for r in range(R):\n",
    "        # Forward pass and compute loss\n",
    "        y_hat = model.forward(train_X)\n",
    "        loss = compute_loss(train_y, y_hat)\n",
    "        #print(f\"Iteration {r}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Compute gradients via backpropagation\n",
    "        dL_dW1, dL_dW2 = model.emp_loss_grad(train_X, train_y)\n",
    "        \n",
    "        # Update velocities and then weights\n",
    "        v_W1 = momentum * v_W1 + lr * dL_dW1\n",
    "        v_W2 = momentum * v_W2 + lr * dL_dW2\n",
    "        model.layers[0].W -= v_W1\n",
    "        model.layers[1].W -= v_W2\n",
    "        \n",
    "        if abs(prev_loss - loss) < 1e-6:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained model\n",
    "trained_model = sgd_momentum(model, X_train, y_train.reshape(1, -1), lr=0.01, momentum=0.9, R=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and prints the accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "      model: Your NN2 model after training.\n",
    "      X_test: Test inputs in shape (K, N) where K is number of features and N is number of test samples.\n",
    "      y_test: True labels for the test data with shape (1, N) or (N,).\n",
    "    \n",
    "    Returns:\n",
    "      accuracy: The classification accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    # Forward pass on test data\n",
    "    y_pred = model.forward(X_test)\n",
    "    \n",
    "    # Convert predicted probabilities to binary labels using a threshold of 0.5\n",
    "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Ensure y_test is in the correct shape\n",
    "    y_test = y_test.reshape(1, -1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(y_pred_labels == y_test) * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.13%\n"
     ]
    }
   ],
   "source": [
    "accuracy = test_model(trained_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Admam Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(model, train_X, train_y, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, R=100):\n",
    "    \"\"\"\n",
    "    Adam optimizer for training the model.\n",
    "    \n",
    "    Parameters:\n",
    "      model: NN2 model.\n",
    "      train_X: input data of shape (K, N) where K is the number of features and N is the number of samples.\n",
    "      train_y: true labels with shape (1, N).\n",
    "      lr: learning rate.\n",
    "      beta1: exponential decay rate for the first moment estimates.\n",
    "      beta2: exponential decay rate for the second moment estimates.\n",
    "      epsilon: small constant for numerical stability.\n",
    "      R: number of iterations.\n",
    "      \n",
    "    Returns:\n",
    "      model: the updated model after training.\n",
    "    \"\"\"\n",
    "    # Initialize the first moment (m) and second moment (v) for each weight matrix.\n",
    "    m_W1 = np.zeros_like(model.layers[0].W)\n",
    "    v_W1 = np.zeros_like(model.layers[0].W)\n",
    "    m_W2 = np.zeros_like(model.layers[1].W)\n",
    "    v_W2 = np.zeros_like(model.layers[1].W)\n",
    "    \n",
    "    t = 0\n",
    "    prev_loss = float('inf')\n",
    "    \n",
    "    for r in range(R):\n",
    "        t += 1\n",
    "        \n",
    "        # Forward pass and compute the loss.\n",
    "        y_hat = model.forward(train_X)\n",
    "        loss = compute_loss(train_y, y_hat)\n",
    "        #print(f\"Iteration {r}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Compute gradients via backpropagation.\n",
    "        dL_dW1, dL_dW2 = model.emp_loss_grad(train_X, train_y)\n",
    "        \n",
    "        # Update biased first moment estimates.\n",
    "        m_W1 = beta1 * m_W1 + (1 - beta1) * dL_dW1\n",
    "        m_W2 = beta1 * m_W2 + (1 - beta1) * dL_dW2\n",
    "        \n",
    "        # Update biased second moment estimates.\n",
    "        v_W1 = beta2 * v_W1 + (1 - beta2) * (dL_dW1 ** 2)\n",
    "        v_W2 = beta2 * v_W2 + (1 - beta2) * (dL_dW2 ** 2)\n",
    "        \n",
    "        # Compute bias-corrected first moment estimates.\n",
    "        m_W1_hat = m_W1 / (1 - beta1 ** t)\n",
    "        m_W2_hat = m_W2 / (1 - beta1 ** t)\n",
    "        \n",
    "        # Compute bias-corrected second moment estimates.\n",
    "        v_W1_hat = v_W1 / (1 - beta2 ** t)\n",
    "        v_W2_hat = v_W2 / (1 - beta2 ** t)\n",
    "        \n",
    "        # Update weights.\n",
    "        model.layers[0].W -= lr * m_W1_hat / (np.sqrt(v_W1_hat) + epsilon)\n",
    "        model.layers[1].W -= lr * m_W2_hat / (np.sqrt(v_W2_hat) + epsilon)\n",
    "        \n",
    "        # Optional: Early stopping if the loss change is small.\n",
    "        if abs(prev_loss - loss) < 1e-6:\n",
    "            break\n",
    "        prev_loss = loss\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X_train is transposed to shape (785, n_train) and y_train is reshaped to (1, n_train)\n",
    "trained_model = adam(model, X_train, y_train.reshape(1, -1), lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, R=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = test_model(trained_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarise: The predictions of my model are:  GD is 81.25% , SGD is 87.13%, Adam is 97.95%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
